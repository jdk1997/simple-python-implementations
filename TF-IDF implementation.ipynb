{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_Instructions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV82tg1xLi0"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUsYm9wjxLi1"
      },
      "source": [
        "## SkLearn# Collection of string documents\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwmFZfKxLi4"
      },
      "source": [
        "### SkLearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4dfQOkxLi4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Om8YpYxLi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c41cb0d-fe9a-41e3-eb3a-8fab5783e781"
      },
      "source": [
        "# sklearn feature names, they are sorted in alphabetic order by default.\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTKplK96xLi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4721f660-929e-415d-c0d4-250e4c46fad9"
      },
      "source": [
        "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
        "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
        "\n",
        "print(vectorizer.idf_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTiWHygxLjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050cd0db-7ec8-47b5-a3f1-75f0bae4adbd"
      },
      "source": [
        "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
        "\n",
        "skl_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDKEpbA-xLjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f41ff8-f58a-4b3a-8690-ef1d3674ef3f"
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# Here the output is a sparse matrix\n",
        "\n",
        "print(skl_output[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QWo34hexLjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0320f317-fca3-4557-8f89-b9ad2343a2e7"
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# convert the sparse output matrix to dense matrix and printing it.\n",
        "# output is normalized using L2 normalization. sklearn does this by default.\n",
        "\n",
        "print(skl_output[0].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIwx5LzxLjI"
      },
      "source": [
        "### Custom implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS0zzB7qS0da"
      },
      "source": [
        "**Fit Method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjuCcJwXxLjJ"
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy\n",
        "\n",
        "def fit(dataset):    \n",
        "    unique_words = set() # initialize an empty set\n",
        "    \n",
        "    # check if its list type or not\n",
        "    if isinstance(dataset, (list,)):\n",
        "        \n",
        "        for row in dataset:    # for each document in the corpus\n",
        "            for word in row.split(\" \"):    # for each word in the document.\n",
        "                \n",
        "                if len(word) < 2:    # if length of word is less than 2, we are not doing anything. We just move to the next iteration.\n",
        "                    continue\n",
        "                \n",
        "                unique_words.add(word)    # length of word is more than 2, add it to the set initialized earlier.\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}   # create a dictionary with word as key and corresponding index as its value. \n",
        "        \n",
        "        return vocab\n",
        "    else:\n",
        "        print(\"Please pass a list of sentence\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dkfDe6gREjg",
        "outputId": "550b5d22-f1cf-4450-a2a5-fa5724385292"
      },
      "source": [
        "vocabulary = fit(corpus)\n",
        "print(vocabulary)\n",
        "for i in vocabulary.keys():\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
            "and\n",
            "document\n",
            "first\n",
            "is\n",
            "one\n",
            "second\n",
            "the\n",
            "third\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tw3oGBuS6r7"
      },
      "source": [
        "**Transform Method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThPaOgQpTBiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13843fa-eb78-418d-f03b-44358f233a1d"
      },
      "source": [
        "def idf(dataset, vocab):\n",
        "    vocabIDF = []\n",
        "    numDocs = len(dataset)\n",
        "    for word in vocab.keys():\n",
        "        cnt = 0\n",
        "        for doc in dataset:\n",
        "            wordsInDoc = doc.split(\" \")\n",
        "            if word in wordsInDoc:\n",
        "                cnt += 1\n",
        "        vocabIDF.append(round((math.log((1 + numDocs)/(cnt + 1))) + 1, 8))\n",
        "    return vocabIDF\n",
        "\n",
        "IDFs = idf(corpus, vocabulary)\n",
        "print(\"IDF values:\")\n",
        "print(IDFs)\n",
        "print()\n",
        "\n",
        "def transform(dataset, fitVocab):\n",
        "    rows = []\n",
        "    columns = []\n",
        "    values = []\n",
        "    if isinstance(dataset, (list,)):\n",
        "        for idx, row in enumerate(tqdm(dataset)):\n",
        "            \n",
        "            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n",
        "            word_freq = dict(Counter(row.split()))\n",
        "            \n",
        "            # length of document will be used later to calculate Term freq(TF).\n",
        "            docLen = len(row.split())\n",
        "\n",
        "            # for every unique word in the document\n",
        "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "\n",
        "                # we will check if word is there in the vocabulary that we build in fit() function\n",
        "                col_index = fitVocab.get(word, -1)    # retreving the dimension number of a word\n",
        "\n",
        "                # if the word exists\n",
        "                if col_index !=-1:\n",
        "                    # we are storing the index of the document\n",
        "                    rows.append(idx)\n",
        "                    \n",
        "                    # we are storing the dimensions of the word\n",
        "                    columns.append(col_index)\n",
        "                    \n",
        "                    tf = freq / docLen\n",
        "                    tf_idf = tf * IDFs[fitVocab[word]]\n",
        "                    \n",
        "                    # we are storing the TF-IDF values of the word\n",
        "                    values.append(tf_idf)\n",
        "        corpusMatrix = csr_matrix((values, (rows,columns)), shape=(len(dataset),len(fitVocab)))\n",
        "\n",
        "        # L2 normalization\n",
        "        normalizedCorpusMatrix = normalize(corpusMatrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "        return normalizedCorpusMatrix\n",
        "    else:\n",
        "        print(\"You need to pass list of strings\")\n",
        "\n",
        "vocabulary = fit(corpus)\n",
        "print(\"Vocab:\")\n",
        "print(vocabulary)\n",
        "print()\n",
        "\n",
        "tfidfMatrix = transform(corpus, vocabulary)\n",
        "\n",
        "print()\n",
        "print(tfidfMatrix[0])   \n",
        "\n",
        "print()\n",
        "print(tfidfMatrix[0].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF values:\n",
            "[1.91629073, 1.22314355, 1.51082562, 1.0, 1.91629073, 1.91629073, 1.0, 1.91629073, 1.0]\n",
            "\n",
            "Vocab:\n",
            "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 5445.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  (0, 1)\t0.46979138558088085\n",
            "  (0, 2)\t0.5802858228626505\n",
            "  (0, 3)\t0.3840852413282814\n",
            "  (0, 6)\t0.3840852413282814\n",
            "  (0, 8)\t0.3840852413282814\n",
            "\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}